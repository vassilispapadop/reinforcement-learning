\documentclass[10pt,a4paper,twocolumn]{article}

\title{On Markov Decision Process, Value Iteration and A*}
\author{Vasileios Papadopoulos}


\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {./images/} }


\begin{document}
	\maketitle
	
	\textbf{Abstract}
	Reinforcement learning is the science of learning to make decisions. In order to make such decisions, the agent has to learn either a policy,	a value function and/or a model. The importance of decision making comes to the affect it has on future expected rewards, agent states and perharps in enviroment states. In this article, we will discuss the mathematical formulation of agent-enviroment interaction know as Markov Decision Process (MDP) as well the Value iteration algorithm which helps us to calculate the value of each possible state the agent can live. In the last part, we will use A* algorithm to find the best possible path in a small 3x4 grid world using value-iteration output as heuristic function.
	
	
	\section{Environment}
	
	Assume a fully observable, non-deterministic 3x4 grid world. In such stochastic environment, 
	each action an agent performs has a certain probability P(a) to succeed(to go as planned) and 1-P(a) to move to a different direction. 
	For example: if the agent wants to go \textit{up}, there is a 0.8 probability to actually move \textit{up} and 0.2 probability to either move \textit{left} or \textit{right}. If the agent wants to move \textit{right} it will succeed with 0.8 probability and with 0.2 it will either move \textit{left} or it will stay and the same state if it hits the wall. Thus, the transition model can be summarized as a probability P(a) to move to desire direction and 1-P(a) to land to state which is perpendicular of the itentended action. Figure below depicts 3x4 grid world and the transition model.
\begin{figure}[ht!]
	\centering
	\includegraphics[width=90mm]{grid_world}
	\caption{3x4 grid world and transition model \label{overflow}}
\end{figure}
	
	

	In case there is a wall in the direction that the agents wishes to move, then it stays put.
	The agent is in constant feedback loop with the environment meaning it takes an action $\alpha$ in state s and lands in a stochastic manner in state s’ and gets a reward r.
	
	The above desicion making problem can be formulated mathematically using Markov Decision Process which will discuss in next section.
	
	\section{Markov Decision Process (MDP)}
	
	Markov Decision process can formally describe the interaction between an agent and the environment. MDPs rely on so-called Markov property, which states the the future is independent of the past given the current/present state. Intuitevely, markov property tells us that all information gathered from previous states can be consider as irelevant as the information of the current state is sufficient. We can formulate this definition as:
	
	A state $s$ has a Markov property if for all states $\forall{s'} \in S$ and all rewards $r \in R$
	
	\begin{equation}
		\begin{split}
			p(R_{t+1} = r , S_{t+1}=s' | S_{t=s}) = \\ 
		 	p(R_{t+1} = r , S_{t+1}=s' | S_{1} ... S_{t-1}, S_{t})
		\end{split}
	\end{equation}
	for all possible histories $ S_{1} ... S_{t-1}, S_{t} $
	
	
	In Markov decision process problems the goal is to find an optimal policy $\pi$* that gives  the best action for each state. Optimal policy $\pi$* maximizes the expected sum of discounted(or not) rewards.
	
	An MDP is defined by the following components:
	\begin{enumerate}
		\item Set of possible states: $S = \{ s_{0}, s_{1}, ..., s_{n} \}$
		\item Initial state: $S_{0}$
		\item Set of possible actions: $A = \{ a_{1}, a{2}, ..., a_{m} \}$
		\item Transition model: $T(s, a, s')$
		\item Reward function: $R(s)$
	\end{enumerate}

	In its general form, the agent operates in a stochastic environment thus we model the non-determistic process with a transition model such as $T(s,a,s')$. However, in real world applications such a model is unknown, the agent does not know state transition probabilities or rewards and it only discover them by taking an action on certain state and receives the reward of the resultant state. This approach is called Q-Learning and it a model-free learning.
	
	\section{Value Iteration}
	Value iteration or Bellman update is a recursive dynamic programming algorithm. It is a method of computing the best values for each possible state an agent can be and eventually extracting the optimal policy $\pi$* for an MDP problem. The agent takes an action $\alpha$ from state s and it lands in state s’ with a probability as this given by transition model . 
	
	\begin{equation}
		V(s) \leftarrow \max_{a}{ \sum_{s'}T(s,a,s') [R(s,a,s') + V(s')]}
	\end{equation}
	
	Intuitively, the value of V(s) is the best action that maximizes the expected reward. Consider the grid world we defined before. In order to compute the value state of cell(3,3) we need to calculate all the possible future rewards for every possible action $A = \{ a_{1}, a{2}, ..., a_{m} \}$, with discount factor $\gamma = 0.9$.
	
	
	\begin{enumerate}
		\item Agent tries to go right: 
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{right} = \sum_{s'} T(\big \langle 3,3 \big \rangle, right, s') [R(\big \langle 3,3 \big \rangle), \gamma V(s')]$
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{right} = 0.9[0.8 * 1 + 0.1 * 0 + 0.1 * 0]$
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{right} = 0.72$
		\end{itemize}
		
		\item Agent tries to go left: 
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{left} = \sum_{s'} T(\big \langle 3,3 \big \rangle, left, s') [R(\big \langle 3,3 \big \rangle), \gamma V(s')]$
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{left} = 0.9[0.8 * 0 + 0.1 * 0 + 0.1 * 0]$
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{left} = 0$
		\end{itemize}
	
		\item Agent tries to go down: 
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{down} = \sum_{s'} T(\big \langle 3,3 \big \rangle, down, s') [R(\big \langle 3,3 \big \rangle), \gamma V(s')]$
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{down} = 0.9[0.8 * 0 + 0.1 * 0 + 0.1 * 1]$
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{down} = 0.09$
		\end{itemize}
	
		\item Agent tries to go up: 
		\begin{itemize}
			
			\item 
			\begin{split}
				$V(\big \langle 3,3 \big \rangle)_{up} = \sum_{s'} T(\big \langle 3,3 \big \rangle, up, s') [R(\big \langle 3,3 \big \rangle), \gamma V(s')]$	
			\end{split}
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{up} = 0.9[0.8 * 0 + 0 * 0 + 0.1 * 1]$
		\end{itemize}
		\begin{itemize}
			\item $V(\big \langle 3,3 \big \rangle)_{up} = 0.09$
		\end{itemize}
	\end{enumerate}

	Then we take the action that maximizes the value state. 
		
	\begin{itemize}
		\item $V(\big \langle 3,3 \big \rangle) = \max_{a} [V(\big \langle 3,3 \big 
		\rangle)_{right},V(\big \langle 3,3 \big \rangle)_{left}, V(\big \langle 3,3 \big \rangle)_{down}, V(\big \langle 3,3 \big \rangle)_{up}]$
	\end{itemize}
		\begin{itemize}
		\item $V(\big \langle 3,3 \big \rangle) = 0.72$
	\end{itemize}
	The above process is repeated for all states. After having calculated all values we repeat again until convergence which is guaranteed by Value iteration algorithm
	\subsection{Pseudo code}
	Below we present the pseudo code algorigthm of Value Iteration.
	\newline
	Initialize array V with zeros ( V(s) = 0 )
	\newline
	Repeat:
	\newline
	$\delta \leftarrow 0$
	\newline
	$\text{For each s} \in S:$
	\newline
	$u \leftarrow V(s)$
	\newline
	$V(s) \leftarrow \max{ ( R(s,a) + \gamma \sum_{s'}T(s,a,s')V(s') )}$
	\newline
	$\delta \leftarrow \max{\delta, | u - V(s)|}$
	\newline
	Until $\delta < \theta$ (small positive number)
	
	
	\subsection{Discount factor}
	 One  important parameter for solving  MDPs is the discount factor $\gamma$. Discount factors are important in MDPs, in a sense they determine how the reward is counted in future states.
	 In this section, we will evaluate the impact discount factor has, to value of each state and ultimately to optimal policy $\pi^{*} $ after the convergece of value iteration algorithm.
	 
	\begin{center}
		\begin{tabular}{ c c c c}
			 \hline
			\multicolumn{4}{|c|}{  \text{$\gamma = 0.9$} } \\
			\hline
			0.61  & 0.77  & 0.93 & 1.00 \\ 
			0.49  & 0.00  & 0.58  & 1.00 \\  
			0.37 &  0.33 & 0.43 & 0.19    
		\end{tabular}
	\end{center}
	
	
		\begin{center}
		\begin{tabular}{ c c c c}
						 \hline
			\multicolumn{4}{|c|}{  \text{$\gamma = 0.6$} } \\
			\hline
			0.18 & 0.42 & 0.86 &1.00 \\ 
			0.05 & 0.00 & 0.30 & 1.00|  \\  
			0.37 &  0.33 & 0.43 & 0.19    
		\end{tabular}
	\end{center}


	\begin{center}
	\begin{tabular}{ c c c c}
					 \hline
		\multicolumn{4}{|c|}{  \text{$\gamma = 0.2$} } \\
		\hline
		0.61  & 0.77  & 0.93 & 1.00 \\ 
		0.49  & 0.00  & 0.58  & 1.00 \\  
		-0.02 & 0.01 &  0.10 & -0.07
	\end{tabular}
\end{center}




	\section{A* Algorithm}
	A* is a graph traversal and path search algorithm. It differs from Dijkstra algorithm as it uses best first search taking into account the current cost g and heuristic function h. It gives priority to nodes that are supposed to be better than others according to the value of function f(s) = g(s) + h(s) where h(s) is a heuristic function. For h(s) = 0, A* is similar to Dijkstra.
	
	\subsection{Pseudo code}
	openSet := {start}
	\newline
	cameFrom := an empty map
	\newline
	gScore := map with default value of Infinity
	\newline
	gScore[start] := 0
	\newline
	fScore := map with default value of Infinity
	\newline
	fScore[start] := h(start)
	\newline
	 while openSet is not empty
	\newline
	current := the node in openSet having the lowest fScore[] value
		\newline
	if current = goal
		\newline
	return reconstructPath(cameFrom, current)
		\newline
	openSet.Remove(current)
		\newline
	for each neighbor of current
	\newline
	tentativeGScore := gScore[current] + d(current, neighbor)
		\newline
	if tentativeGScore < gScore[neighbor]
		\newline

	cameFrom[neighbor] := current
		\newline
	gScore[neighbor] := tentative_gScore
		\newline
	fScore[neighbor] := gScore[neighbor] + h(neighbor)
		\newline
	if neighbor not in openSet
		\newline
	openSet.add(neighbor)
		\newline

	return failure
	
	\section{Conclusion}
	In this article we briefly presented Markov Decision Process problems and the challenge to find the optimal policy for each state an agent could be. We used Value Iteration algorithm to calculate the value of each state and explored the impact of discount factor $\gamma$. Lastly, we implemented A* path finding algorithm and used the converged state-values as heuristic.
	\section{References}
	
	
	\begin{thebibliography}{9}
		\bibitem{latexcompanion} 
		Michel Goossens, Frank Mittelbach, and Alexander Samarin. 
		\textit{The \LaTeX\ Companion}. 
		Addison-Wesley, Reading, Massachusetts, 1993.
		
		\bibitem{einstein} 
		Albert Einstein. 
		\textit{Zur Elektrodynamik bewegter K{\"o}rper}. (German) 
		[\textit{On the electrodynamics of moving bodies}]. 
		Annalen der Physik, 322(10):891–921, 1905.
		
		\bibitem{knuthwebsite} 
		Knuth: Computers and Typesetting,
		\\\texttt{http://www-cs-faculty.stanford.edu/\~{}uno/abcde.html}
	\end{thebibliography}
	
\end{document}